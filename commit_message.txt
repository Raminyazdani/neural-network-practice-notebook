Ramin Yazdani | Neural Network MNIST Classifier | main | WIP(model): Implement cross-entropy loss function

Added cross-entropy loss function to measure prediction error and track training
progress. This loss function is the standard choice for multi-class classification
with softmax output.

What was added:
- compute_loss(y_true, y_pred) method
- Cross-entropy formula: -mean(sum(y_true * log(y_pred + epsilon)))
- Numerical stability: epsilon=1e-10 prevents log(0) errors
- Returns scalar loss value for monitoring

Rationale: The loss function quantifies how wrong the model's predictions are.
Cross-entropy is optimal for softmax outputs as it measures the difference between
predicted probability distribution and true distribution. Will be used to monitor
training convergence and as the objective function for gradient descent.
