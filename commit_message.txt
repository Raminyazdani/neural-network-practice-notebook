Ramin Yazdani | Neural Network MNIST Classifier | main | WIP(model): Add softmax activation and forward propagation

Completed the forward propagation implementation by adding softmax activation
and the full forward pass through both layers of the network. This enables the
network to make predictions.

What was added:
- Softmax activation implementation:
  - Numerically stable version with exp shift to prevent overflow
  - Converts logits to probability distribution over 10 classes
- Forward propagation method:
  - Hidden layer: Z1 = X·W1 + b1, A1 = ReLU(Z1)
  - Output layer: Z2 = A1·W2 + b2, A2 = softmax(Z2)
  - Returns intermediate values (Z1, A1, Z2, A2) needed for backpropagation

Rationale: Softmax is the standard activation for multi-class classification.
Forward propagation computes predictions and caches intermediate values needed
for efficient backpropagation gradient computation.
