Ramin Yazdani | Neural Network MNIST Classifier | main | WIP(model): Implement neural network class with initialization and ReLU

Started building the TwoLayerNN class with proper weight initialization and ReLU
activation function. This establishes the neural network architecture foundation
before implementing the full forward pass.

What was added:
- TwoLayerNN class with __init__ method:
  - Xavier/Glorot initialization for weights (prevents vanishing/exploding gradients)
  - Zero initialization for biases
  - Architecture: 784 inputs → 64 hidden units → 10 outputs
- ReLU activation: relu(Z) = max(0, Z)
- ReLU derivative: relu_derivative(Z) = 1 if Z > 0 else 0

Rationale: Build incrementally by starting with initialization and the first
activation function. ReLU is used in the hidden layer for non-linearity while
being computationally efficient and avoiding vanishing gradient issues.
