{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "27f0490b-717f-46e4-81a5-1e499de95742",
      "metadata": {},
      "source": [
        "# MNIST Digit Classifier: Two-Layer Neural Network\n",
        "\n",
        "Building a neural network from scratch using NumPy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bc1f734-9e54-4204-b367-b2cbbd4eee37",
      "metadata": {},
      "source": [
        "## Data Loading and Preprocessing\n",
        "\n",
        "Loading the MNIST dataset and preparing it for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "323945ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load MNIST dataset\n",
        "def load_mnist():\n",
        "    mnist = fetch_openml('mnist_784', version=1)\n",
        "    X, y = mnist.data / 255.0, mnist.target.astype(int)\n",
        "    return X, y.to_numpy()  # Convert y to a NumPy array\n",
        "\n",
        "# One-hot encode labels\n",
        "def one_hot_encode(y, num_classes):\n",
        "    encoder = OneHotEncoder(sparse_output=False, categories=[range(num_classes)])\n",
        "    return encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "# Split dataset\n",
        "def prepare_data(test_size=0.2):\n",
        "    X, y = load_mnist()\n",
        "    y_encoded = one_hot_encode(y, num_classes=10)\n",
        "    return train_test_split(X, y_encoded, test_size=test_size, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = prepare_data()\n",
        "X_train, X_test = X_train.to_numpy(), X_test.to_numpy()\n",
        "print(f\"Training Data Shape: {X_train.shape}, Test Data Shape: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9156fa0-70dc-4330-a267-47cc0bf8f399",
      "metadata": {},
      "source": [
        "## Neural Network Implementation\n",
        "\n",
        "Implementing the two-layer neural network architecture with forward propagation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39532323",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class TwoLayerNN:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        \"\"\"\n",
        "        Initialize weights and biases.\n",
        "        \"\"\"\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "\n",
        "    def relu(self, Z):\n",
        "        \"\"\"\n",
        "        ReLU activation function.\n",
        "        \"\"\"\n",
        "        return np.maximum(0, Z)\n",
        "\n",
        "    def relu_derivative(self, Z):\n",
        "        \"\"\"\n",
        "        Derivative of ReLU activation.\n",
        "        \"\"\"\n",
        "        return (Z > 0).astype(float)\n",
        "\n",
        "    def softmax(self, Z):\n",
        "        \"\"\"\n",
        "        Softmax activation function.\n",
        "        \"\"\"\n",
        "        expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
        "        return expZ / np.sum(expZ, axis=1, keepdims=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        \"\"\"\n",
        "        self.Z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.A1 = self.relu(self.Z1)\n",
        "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
        "        self.A2 = self.softmax(self.Z2)\n",
        "        return self.A2\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Compute cross-entropy loss.\n",
        "        \"\"\"\n",
        "        m = y_true.shape[0]\n",
        "\n",
        "        # Handle both one-hot and class index labels\n",
        "        if len(y_true.shape) > 1:\n",
        "            y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "        log_likelihood = -np.log(y_pred[range(m), y_true])\n",
        "        loss = np.sum(log_likelihood) / m\n",
        "        return loss\n",
        "\n",
        "    def backward(self, X, y):\n",
        "        \"\"\"\n",
        "        Backpropagation to compute gradients.\n",
        "        Return: dW1, db1, dW2, db2\n",
        "        \"\"\"\n",
        "        m = X.shape[0]\n",
        "\n",
        "        # Handle both one-hot and class index labels\n",
        "        if len(y.shape) > 1:\n",
        "            y = np.argmax(y, axis=1)\n",
        "\n",
        "        y_one_hot = np.zeros_like(self.A2)\n",
        "        y_one_hot[range(m), y] = 1\n",
        "\n",
        "        dZ2 = self.A2 - y_one_hot\n",
        "        dW2 = np.dot(self.A1.T, dZ2) / m\n",
        "        db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "\n",
        "        dA1 = np.dot(dZ2, self.W2.T)\n",
        "        dZ1 = dA1 * self.relu_derivative(self.Z1)\n",
        "        dW1 = np.dot(X.T, dZ1) / m\n",
        "        db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "\n",
        "        return dW1, db1, dW2, db2"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}