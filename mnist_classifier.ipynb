{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "27f0490b-717f-46e4-81a5-1e499de95742",
      "metadata": {},
      "source": [
        "# MNIST Digit Classifier: Two-Layer Neural Network\n",
        "\n",
        "Building a neural network from scratch using NumPy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bc1f734-9e54-4204-b367-b2cbbd4eee37",
      "metadata": {},
      "source": [
        "## Data Loading and Preprocessing\n",
        "\n",
        "Loading the MNIST dataset and preparing it for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "323945ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load MNIST dataset\n",
        "def load_mnist():\n",
        "    mnist = fetch_openml('mnist_784', version=1)\n",
        "    X, y = mnist.data / 255.0, mnist.target.astype(int)\n",
        "    return X, y.to_numpy()  # Convert y to a NumPy array\n",
        "\n",
        "# One-hot encode labels\n",
        "def one_hot_encode(y, num_classes):\n",
        "    encoder = OneHotEncoder(sparse_output=False, categories=[range(num_classes)])\n",
        "    return encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "# Split dataset\n",
        "def prepare_data(test_size=0.2):\n",
        "    X, y = load_mnist()\n",
        "    y_encoded = one_hot_encode(y, num_classes=10)\n",
        "    return train_test_split(X, y_encoded, test_size=test_size, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = prepare_data()\n",
        "X_train, X_test = X_train.to_numpy(), X_test.to_numpy()\n",
        "print(f\"Training Data Shape: {X_train.shape}, Test Data Shape: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9156fa0-70dc-4330-a267-47cc0bf8f399",
      "metadata": {},
      "source": [
        "## Neural Network Implementation\n",
        "\n",
        "Implementing the two-layer neural network architecture with forward propagation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39532323",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class TwoLayerNN:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        \"\"\"\n",
        "        Initialize weights and biases.\n",
        "        \"\"\"\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "\n",
        "    def relu(self, Z):\n",
        "        \"\"\"\n",
        "        ReLU activation function.\n",
        "        \"\"\"\n",
        "        return np.maximum(0, Z)\n",
        "\n",
        "    def relu_derivative(self, Z):\n",
        "        \"\"\"\n",
        "        Derivative of ReLU activation.\n",
        "        \"\"\"\n",
        "        return (Z > 0).astype(float)\n",
        "\n",
        "    def softmax(self, Z):\n",
        "        \"\"\"\n",
        "        Softmax activation function.\n",
        "        \"\"\"\n",
        "        expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
        "        return expZ / np.sum(expZ, axis=1, keepdims=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        \"\"\"\n",
        "        self.Z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.A1 = self.relu(self.Z1)\n",
        "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
        "        self.A2 = self.softmax(self.Z2)\n",
        "        return self.A2\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Compute cross-entropy loss.\n",
        "        \"\"\"\n",
        "        m = y_true.shape[0]\n",
        "\n",
        "        # Handle both one-hot and class index labels\n",
        "        if len(y_true.shape) > 1:\n",
        "            y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "        log_likelihood = -np.log(y_pred[range(m), y_true])\n",
        "        loss = np.sum(log_likelihood) / m\n",
        "        return loss\n",
        "\n",
        "    def backward(self, X, y):\n",
        "        \"\"\"\n",
        "        Backpropagation to compute gradients.\n",
        "        Return: dW1, db1, dW2, db2\n",
        "        \"\"\"\n",
        "        m = X.shape[0]\n",
        "\n",
        "        # Handle both one-hot and class index labels\n",
        "        if len(y.shape) > 1:\n",
        "            y = np.argmax(y, axis=1)\n",
        "\n",
        "        y_one_hot = np.zeros_like(self.A2)\n",
        "        y_one_hot[range(m), y] = 1\n",
        "\n",
        "        dZ2 = self.A2 - y_one_hot\n",
        "        dW2 = np.dot(self.A1.T, dZ2) / m\n",
        "        db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "\n",
        "        dA1 = np.dot(dZ2, self.W2.T)\n",
        "        dZ1 = dA1 * self.relu_derivative(self.Z1)\n",
        "        dW1 = np.dot(X.T, dZ1) / m\n",
        "        db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "\n",
        "        return dW1, db1, dW2, db2",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels.\n",
        "        \"\"\"\n",
        "        probabilities = self.forward(X)\n",
        "        return np.argmax(probabilities, axis=1)\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        \"\"\"\n",
        "        Train the model using gradient descent.\n",
        "        \"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            y_pred = self.forward(X)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = self.compute_loss(y, y_pred)\n",
        "\n",
        "            # Backward pass\n",
        "            dW1, db1, dW2, db2 = self.backward(X, y)\n",
        "\n",
        "            # Update parameters\n",
        "            self.W1 -= learning_rate * dW1\n",
        "            self.b1 -= learning_rate * db1\n",
        "            self.W2 -= learning_rate * dW2\n",
        "            self.b2 -= learning_rate * db2\n",
        "\n",
        "            # Print loss every 10 epochs or last epoch\n",
        "            if epoch % 10 == 0 or epoch == epochs - 1:\n",
        "                print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c4df5d7-1f43-4a1c-88a5-1e970816dec7",
      "metadata": {},
      "source": [
        "## Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d066af0c-dce7-41d6-b928-104e2691a5be",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size = 64  # You can choose a suitable value\n",
        "output_size = 10  # Number of classes\n",
        "\n",
        "model = TwoLayerNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# Training the model\n",
        "epochs = 100\n",
        "learning_rate = 0.5\n",
        "model.train(X_train, y_train, epochs, learning_rate)\n",
        "\n",
        "# Evaluate on test data\n",
        "predictions = model.predict(X_test)\n",
        "accuracy = np.mean(predictions == np.argmax(y_test, axis=1))\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}